{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"auxf.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"pfXNp4hO-uPr","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"error","timestamp":1592650790719,"user_tz":-60,"elapsed":2619,"user":{"displayName":"Helder Daniel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9_X9PHXyfxsFqdQBJarSFy5TpLITVEg043ihVUA=s64","userId":"03611350071843918342"}},"outputId":"7c743cff-b3d3-4c97-ea11-5cbb1e70512b"},"source":["import tensorflow as tf\n","import numpy as np\n","import hdf5storage as hdf\n","import sklearn.preprocessing as process\n","\n","print('aux 1.15.34')\n","\n","if useColab:\n","  %run gdrive/My\\ Drive/Colab\\ Notebooks/lib/model.ipynb\n","else:\n","  %run lib/model.ipynb"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6f14f25f4831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdf5storage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdf5storage'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Hht8FDXF-uPz","colab":{}},"source":["#convert to one-hot encoding\n","#https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n","def one_hot(labels, nclasses):\n","    \"\"\"\n","    One-hot encoding\n","\n","    labels:   a vector of labels\n","    nclasses: number of classes\n","\n","    Note:\n","    to_categorical() is doing:\n","    \n","    y=np.zeros((labels.size, nclasses))\n","    i=0\n","    for l in labels:\n","        y[i,l] = 1\n","        i+=1\n","    \n","    Note: to decode to integer value again use:\n","\n","    valueInteger = numpy.argmax(encoded[0])\n","    \"\"\"\n","    #2nd arg: nclasses can be omitted if labels have all the possible values\n","    y = tf.keras.utils.to_categorical(labels, nclasses)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kzfxGRs6-uP2","colab":{}},"source":["##########################\n","# load data and resample #\n","##########################\n","def minmax(X):\n","    #normalize min-max\n","    #https://swaathi.com/2017/04/29/normalizing-data/\n","    #[min >= 0, max <=1]\n","    #minimum = X.min()\n","    #maximum = X.max()\n","    #X = (X - minimum) / (maximum - minimum)\n","    \n","    #[0, 1]\n","    X = process.minmax_scale(X)\n","    return X\n","\n","  \n","def quantileTransform(X):\n","    #Normalize Quantile transform\n","    from sklearn.preprocessing.data import QuantileTransformer\n","    qTransform = QuantileTransformer(output_distribution='uniform')\n","    qTransform.fit(X)\n","    X = qTransform.transform(X)\n","    return X\n","\n","\n","#eg. to reduce from 36 to 4 classes: \n","#labels = joinClasses(labels, [(0, 0), (1, 7), (8, 14), (15, 35)])\n","def joinClasses(labels, join):\n","    for i in range(len(labels)):\n","        for j in range(len(join)):\n","            if labels[i] >=  join[j][0] and labels[i] <= join[j][1]:\n","                labels[i] = j\n","    return labels \n","\n","\n","def loadData(filename):\n","     #Get data\n","    return hdf.loadmat(filename) \n","\n","  \n","def getData1(data, underSample=1):\n","    #SetupData\n","    trainXs = np.array(data['features_training'][:, ::underSample])\n","    trainYs = np.array(data['labels_training'], dtype=np.int8)\n","    testXs  = np.array(data['features_test'][:, ::underSample])\n","    testYs  = np.array(data['labels_test'], dtype=np.int8)\n","    \n","    #Normalize\n","    #print('QNormalizing')\n","    #trainXs = minmax(trainXs)\n","    #testXs  = minmax(testXs)\n","    #trainXs = quantileTransform(trainXs)\n","    #testXs  = quantileTransform(testXs)\n","    \n","    return trainXs, trainYs, testXs, testYs\n","\n","\n","def getData2(data, numClasses, trainSize=0.8, underSample=1, \n","             featuresName='X', labelsName='Y', xType='float32', yType='uint8'):  \n","    #Get data\n","    X = np.array(data[featuresName][:, ::underSample])\n","    Y = np.array(data[labelsName], dtype=np.int8)\n","    \n","    #make sure Y as cols dim = 1 and NOT none\n","    #as is the default if not onehot encoded\n","    if len(Y.shape) == 1:\n","        Y = Y.reshape(Y.shape[0], 1)\n","\n","    #Normalize\n","    #print('Normalizing')\n","    #X = minmax(X)\n","    #X = quantileTransform(X)\n","    \n","    #Separate datasets equally by fault classes\n","    sinalLength= int(X.shape[1])\n","    samples    = int(X.shape[0] / numClasses)\n","    splitPoint = int(samples*trainSize)\n","    #print(samples, splitPoint)\n","    \n","    trainXs = []\n","    trainYs = []\n","    testXs  = []\n","    testYs  = []\n","\n","    for i in range(numClasses):\n","        print (i)\n","\n","        #slice fault\n","        st = i*samples\n","        sp = st + splitPoint\n","        end = (i+1)*samples\n","        \n","        #shuffle in place each fault data before slice train/set\n","        p = np.random.permutation(samples)\n","        X[st:end, :] = X[st+p, :]\n","        Y[st:end, :] = Y[st+p, :]\n","        \n","        #print(trainXs.shape, trainYs.shape, testXs.shape, testYs.shape)\n","        #print(X[st:sp, :].shape, Y[st:sp, :].shape, X[sp:end, :].shape, Y[sp:end, :].shape)\n","            \n","        trainXs.append(X[st:sp, :])\n","        trainYs.append(Y[st:sp, :])\n","        testXs.append(X[sp:end, :])\n","        testYs.append(Y[sp:end, :])\n","\n","    #Reshape matrices to proper sizes and define data types\n","    trainXs = np.array(trainXs, dtype=xType)\n","    trainYs = np.array(trainYs, dtype=yType)\n","    testXs  = np.array(testXs,  dtype=xType)\n","    testYs  = np.array(testYs,  dtype=yType)\n","    trainXs = trainXs.reshape(numClasses*splitPoint, sinalLength)\n","    trainYs = trainYs.reshape(numClasses*splitPoint, numClasses)\n","    testXs  = testXs.reshape (numClasses*(samples-splitPoint), sinalLength)\n","    testYs  = testYs.reshape (numClasses*(samples-splitPoint), numClasses)\n","\n","    #Faults are ordered by classes\n","    #this shuffles faults order\n","    #\n","    #Not needed if rfit is set to Shuffle\n","    p = np.random.permutation(trainXs.shape[0])\n","    trainXs = trainXs[p]\n","    trainYs = trainYs[p]\n","    \n","    p = np.random.permutation(testXs.shape[0])\n","    testXs = testXs[p]\n","    testYs = testYs[p]\n","\n","    return trainXs, trainYs, testXs, testYs\n","\n","  \n","  \n","def getData3(data, numClasses, useSize=1, trainSize=0.8, underSample=1, \n","             featuresName='X', labelsName='Y', xType='float32', yType='uint8'):  \n","    #Get data\n","    X = np.array(data[featuresName][:, ::underSample], dtype=xType)\n","    Y = np.array(data[labelsName], dtype=yType)\n","    \n","    #Discard part\n","    useSamples = int(X.shape[0]*useSize)\n","    us = useSamples\n","    X = X[:us, :]\n","    Y = Y[:us, :]\n","    \n","    \n","    #make sure Y as cols dim = 1 and NOT none\n","    #as is the default if not onehot encoded\n","    if len(Y.shape) == 1:\n","        Y = Y.reshape(Y.shape[0], 1)\n","\n","    #Separate datasets\n","    samples    = int(X.shape[0])\n","    splitPoint = int(samples*trainSize)\n","    sp = splitPoint\n","\n","    trainXs = X[:sp, :]\n","    testXs  = X[sp:, :]\n","    trainYs = Y[:sp, :]\n","    testYs  = Y[sp:, :]\n","    \n","    return trainXs, trainYs, testXs, testYs\n","  \n"," \n","\n","def setupData(trainXs, trainYs, testXs, testYs, numClasses=1, samplesSlice=1, samplesPart=0, windowSlice=1, conv2Drows=1, conv2Dcols=1, underSample=1, useCNN='1D', DTYPE=None):\n","    \n","    #get smaller sample set\n","    #just done to halve for now!!!!\n","    if samplesSlice > 1:\n","        if samplesPart == 0:\n","            #lower half\n","            trainXs = trainXs[0:int(trainXs.shape[0]/samplesSlice)]\n","            testXs  = testXs [0:int(testXs.shape[0] /samplesSlice)]\n","            trainYs = trainYs[0:int(trainYs.shape[0]/samplesSlice)]\n","            testYs  = testYs [0:int(testYs.shape[0] /samplesSlice)]\n","        else:\n","            #upper half\n","            trainXs = trainXs[int(trainXs.shape[0]/samplesSlice): ]\n","            testXs  = testXs [int(testXs.shape[0] /samplesSlice): ]\n","            trainYs = trainYs[int(trainYs.shape[0]/samplesSlice): ]\n","            testYs  = testYs [int(testYs.shape[0] /samplesSlice): ]\n","            \n","    #need a 3 dim for CNN. This can be use to put all channels\n","    if useCNN == '1D':\n","        #Pass to Window slice divide Window 50 000 pts by WindowSlice factor\n","        trainXs = trainXs.reshape(int(trainXs.shape[0]*windowSlice), int(trainXs.shape[1]/conv2Dcols/windowSlice), conv2Dcols)\n","        testXs  = testXs.reshape (int(testXs.shape[0]*windowSlice),  int(testXs.shape[1]/conv2Dcols/windowSlice),  conv2Dcols)\n","\n","    if useCNN == '2D':\n","        #Pass to Window slice divide Window 50 000 pts by WindowSlice factor\n","        trainXs = trainXs.reshape(int(trainXs.shape[0]*windowSlice), int(trainXs.shape[1]/conv2Drows/conv2Dcols/windowSlice), conv2Drows, conv2Dcols)\n","        testXs  = testXs.reshape (int(testXs.shape[0]*windowSlice),  int(testXs.shape[1]/conv2Drows/conv2Dcols/windowSlice),  conv2Drows, conv2Dcols)\n","\n","    if useCNN is None: #Dense MLP\n","        trainXs = trainXs.reshape(int(trainXs.shape[0]*windowSlice), int(trainXs.shape[1]/windowSlice))\n","        testXs  = testXs.reshape (int(testXs.shape[0]*windowSlice),  int(testXs.shape[1]/windowSlice))\n","\n","    \n","    #If windowSlice>1 add labels to new windows\n","    trainYs = np.repeat(trainYs, windowSlice, axis=0)\n","    testYs  = np.repeat(testYs,  windowSlice, axis=0)\n","    \n","    \n","    #Just to test time to train each epoch of 100 000 pts windows\n","    #but it is wrong since labels are wrong\n","    #and original signal was shuffled in 50 000 points windows\n","    #trainYs = trainYs[0:int(trainYs.shape[0]/2)]\n","    #testYs  = testYs [0:int(testYs.shape[0]/2)]\n","    \n","    #Set float 16, 32, 64 on supported GPUs\n","    if DTYPE is not None:\n","        tf.keras.backend.set_floatx(DTYPE)\n","        if DTYPE == 'float16':\n","            tf.keras.backend.set_epsilon(1e-4) #default is 1e-7 reduce to avoid NAN (must be tested)\n","\n","        trainXs = tf.keras.backend.cast_to_floatx(trainXs)\n","        testXs  = tf.keras.backend.cast_to_floatx(testXs)\n","        trainYs = tf.keras.backend.cast_to_floatx(trainYs)\n","        testYs  = tf.keras.backend.cast_to_floatx(testYs)\n","        '''\n","        #or:\n","        trainXs = trainXs.astype(DTYPE)\n","        trainYs = trainYs.astype(DTYPE)\n","        testXs  =  testXs.astype(DTYPE)\n","        testYs  =  testYs.astype(DTYPE)\n","        '''\n","\n","    return trainXs, trainYs, testXs, testYs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9PTXrW35-uP5","colab":{}},"source":["################\n","# define model #\n","################\n","def setupModel(inputLen : int, numClasses=1, windowSlice=1, conv1Dcols=1, dropOutRatio=0.5, filters=32, useCNN=True): \n","    if useCNN == True:\n","        #model = cnnModel1(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, filters)\n","        model = cnnModel31(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, 32, 20)\n","        #model = cnnModel32(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, 32, 20)\n","        #model = cnnModel5(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, 32, 20)\n","        #model = cnnModel4(int(inputLen/conv1Dcols/windowSlice, conv1Dcols, numClasses, dropOutRatio, 10, 20)\n","        #model = cnnModel6(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, 16, 20)\n","        #model = cnnModel7(int(inputLen/conv1Dcols/windowSlice), conv1Dcols, numClasses, dropOutRatio, 32, 20)\n","    else:\n","        model = denseModel(inputLen/windowSlice, numClasses) \n","    return model\n","  \n","################\n","# define model #\n","################\n","def setupCNN2D(inputLen, numClasses=1, windowSlice=1, conv2Drows=1, conv2Dcols=1, dropOutRatio=0.5, filters=32): \n","    #model = cnnModel31_2D(int(inputLen/conv2Drows/conv2Dcols/windowSlice), conv2Drows, conv2Dcols, numClasses, dropOutRatio, 32, 20) #use filters (conv2Drows, 20)\n","    model = cnnModel32_2D(int(inputLen/conv2Drows/conv2Dcols/windowSlice), conv2Drows, conv2Dcols, numClasses, dropOutRatio, 32, 20) #use filters (conv2Drows, 20)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nLOUDydL-uP8","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564822228090,"user_tz":-60,"elapsed":1690,"user":{"displayName":"Programação de Sistemas","photoUrl":"","userId":"10716334754187268466"}},"outputId":"040336fe-4f7e-44d8-ae68-451820277f2d"},"source":["#https://github.com/tensorflow/tensorflow/issues/29798\n","\n","def getTPU():\n","    try:\n","        deviceName = os.environ['COLAB_TPU_ADDR']\n","        TPUaddr = 'grpc://' + deviceName\n","        print('Found TPU at: {}'.format(TPUaddr))\n","    except KeyError:\n","        print('TPU not found')\n","        sys.exit(0)\n","    \n","    return TPUaddr\n","  \n","\n","def setupTPUModel114(inputLen, numClasses=1, windowSlice=1, conv1Dcols=1, dropOutRatio=0.5, filters=10, useCNN=True):\n","    TPUaddr = getTPU()\n","    '''\n","    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPUaddr)\n","    tf.contrib.distribute.initialize_tpu_system(resolver)\n","    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n","    '''\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPUaddr)\n","    tf.config.experimental_connect_to_host(resolver.master())\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","    with strategy.scope():\n","        tpuModel = setupModel(inputLen, numClasses, windowSlice, conv1Dcols, dropOutRatio, filters)\n","    \n","    return tpuModel\n","  \n","def setupTPUModel(model):    \n","    TPUaddr = getTPU()\n","    return tf.contrib.tpu.keras_to_tpu_model(\n","        model,\n","        strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","        tf.contrib.cluster_resolver.TPUClusterResolver(\n","        tpu=TPUaddr)\n","        ))\n","   \n","\"\"\"\n","tpuModel = cnnModelTPU(strategy, inputLen/conv1Dcols/windowSlice, conv1Dcols, numClasses, dropOutRatio, filters)\n","with strategy.scope():\n","    tpuModel.compile(optimizer=tf.keras.optimizers.Adam(lr=initialLR), loss='mse', metrics=['acc', 'mae', 'mse'])\n","    #tpuModel.compile(optimizer=tf.keras.optimizers.Adam(lr=initialLR), loss='categorical_crossentropy', metrics=['acc', 'mae', 'mse'])\n","    '''\n","    tpuModel.compile(\n","        optimizer=tf.train.AdamOptimizer(learning_rate=initialLR, ),\n","        #loss=tf.keras.losses.sparse_categorical_crossentropy,\n","        #metrics=['sparse_categorical_accuracy']\n","        #loss=tf.keras.losses.categorical_crossentropy,\n","        #metrics=['categorical_accuracy'])       \n","        loss='mse',\n","        metrics=['acc', 'mae', 'mse']\n","    )\n","    '''\n","\"\"\"\n","    "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntpuModel = cnnModelTPU(strategy, inputLen/conv1Dcols/windowSlice, conv1Dcols, numClasses, dropOutRatio, filters)\\nwith strategy.scope():\\n    tpuModel.compile(optimizer=tf.keras.optimizers.Adam(lr=initialLR), loss='mse', metrics=['acc', 'mae', 'mse'])\\n    #tpuModel.compile(optimizer=tf.keras.optimizers.Adam(lr=initialLR), loss='categorical_crossentropy', metrics=['acc', 'mae', 'mse'])\\n    '''\\n    tpuModel.compile(\\n        optimizer=tf.train.AdamOptimizer(learning_rate=initialLR, ),\\n        #loss=tf.keras.losses.sparse_categorical_crossentropy,\\n        #metrics=['sparse_categorical_accuracy']\\n        #loss=tf.keras.losses.categorical_crossentropy,\\n        #metrics=['categorical_accuracy'])       \\n        loss='mse',\\n        metrics=['acc', 'mae', 'mse']\\n    )\\n    '''\\n\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g82BUObo-uP_","colab":{}},"source":["#################################\n","# train model CPU or GPU or TPU #\n","#################################\n","\n","#Rewrite as in classifier\n","\n","def trainModel(trainXs, trainYs, epochs=100, batchSize=1, callbacks=[], shuffleValData=False):\n","    if shuffleValData:\n","        shuffle = False\n","    else:\n","        shuffle = True\n","\n","    if shuffleValData == False:\n","        trainResponse = model.fit(trainXs, trainYs,\n","                        epochs=epochs,\n","                        batch_size=batchSize,\n","                        shuffle=shuffle,      #shuffle before each epoch (already shuffled)\n","                        validation_split=0.3, #use 30% of samples to validate\n","                        callbacks=callbacks,\n","                        verbose=1, #default\n","                        )\n","    else:\n","        for e in range(epochs):\n","            print(f'Epoch: {e+1:04d}/{epochs:04d}')\n","            print(\"Shuffling train data...\")\n","            p = np.random.permutation(trainXs.shape[0])\n","            #trainResponse = model.fit(trainXs, trainYs,\n","            trainResponse = model.fit(trainXs[p], trainYs[p],\n","                        epochs=1,\n","                        batch_size=batchSize,\n","                        shuffle=shuffle,      #already shuffled\n","                        validation_split=0.3, #use 30% of samples to validate\n","                        callbacks=callbacks,\n","                        verbose=1, #default\n","                        )\n","    return trainResponse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aX95_pIV-uQC","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564822228094,"user_tz":-60,"elapsed":1683,"user":{"displayName":"Programação de Sistemas","photoUrl":"","userId":"10716334754187268466"}},"outputId":"938adb78-457d-4dd1-c29b-121e05daf0bb"},"source":["'''\n","def train_input_fn(trainXs, trainYs, batchSize=1024):\n","    # Convert the inputs to a Dataset.\n","    dataset = tf.data.Dataset.from_tensor_slices((trainXs, trainYs))\n","    # Shuffle, repeat, and batch the examples.\n","    dataset = dataset.cache()\n","    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(batchSize, drop_remainder=True)\n","    # Return the dataset.\n","    return dataset\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef train_input_fn(trainXs, trainYs, batchSize=1024):\\n    # Convert the inputs to a Dataset.\\n    dataset = tf.data.Dataset.from_tensor_slices((trainXs, trainYs))\\n    # Shuffle, repeat, and batch the examples.\\n    dataset = dataset.cache()\\n    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\\n    dataset = dataset.repeat()\\n    dataset = dataset.batch(batchSize, drop_remainder=True)\\n    # Return the dataset.\\n    return dataset\\n'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"quE8X00-NYq3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564822228095,"user_tz":-60,"elapsed":1676,"user":{"displayName":"Programação de Sistemas","photoUrl":"","userId":"10716334754187268466"}},"outputId":"b7a9bc22-d5e2-4f8a-ecad-2e5dbd18f40d"},"source":["#Main\n","'''\n","import numpy as np\n","\n","if __name__ == \"__main__\":\n","    labels = np.random.randint(36, size=100)\n","    print(labels)\n","    labels = joinClasses(labels, [(0, 0), (1, 7), (8, 14), (15, 35)])\n","    print(labels)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport numpy as np\\n\\nif __name__ == \"__main__\":\\n    labels = np.random.randint(36, size=100)\\n    print(labels)\\n    labels = joinClasses(labels, [(0, 0), (1, 7), (8, 14), (15, 35)])\\n    print(labels)\\n'"]},"metadata":{"tags":[]},"execution_count":11}]}]}